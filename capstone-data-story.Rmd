---
title: "THE MELBOURNE DECONSTRUCTION"
date: "February 21, 2018"
output:
  pdf_document: 
    fig_height: 6
  html_document:
    self_contained: no
    smart: no
  word_document: default
subtitle: Capstone Project Data Story
urlcolor: Bittersweet
header-includes:
  - \hypersetup{colorlinks=true,
            allbordercolors={0 0 0},
            pdfborderstyle={/S/U/W 1}}
---
# INTRODUCING THE PROBLEM

In my capstone project, I wanted to answer the following **research question**:

>At my local coffee shop, are **sales of drinks** influenced by **weather**?

The shop's owner was excited to see if that is the case and provided me with daily sales reports in *\*.csv* format---448 of them.[^1]

These reports were generated by a cloud-based cash register system.

The system records sales of items like "latte," "chai latte," or "dirty chai"---names that the manager chose.

[Third wave coffee aficionados](https://www.esquire.com/food-drink/drinks/a24501/third-wave-coffee-ozersky/) aside,[^2] most readers won't be able to explain [what makes these drinks different](https://pbs.twimg.com/media/DSP8agIX0AEHpyl.jpg). ([And we're not even going in too deep yet.](https://www.baristaproshop.com/blog/images/P-Coffee_Zoom.jpg))

**Well, the data I received couldn't do that either.**

>The client supplied me with daily sales reports counting unclear categories.
>
>*How could I reinterpret the data to make it useful?*

I wanted to know if there's a relationship between what drinks the café sells and the weather conditions outside.

But what's the worth in knowing that, say, increased *pressure* and *temperature* are associated with *more sales of "lattes"* or *"mochas"*?

Knowing the answer wouldn't deliver much value to the client in terms of logistics or insight into consumer behavior.

Further, interpreting the results would be a nightmare. If we see a trend for "double-shot lattes," but not regular "lattes," is it the extra shot of espresso that matters? What if we don't see this same trend for "double-shot espressos," which also have the extra shot but nothing else?

The problem is that **by default, a drink is just a name**. 

It's an "americano," a "latte," a "cappuccino," or a "macchiato."

It has to be made meaningful, and the way to do it is to learn *what it's made of* and *how*.

>To produce actionable insights, I'm not thinking in terms of *drinks*.
>
>I'm thinking in terms of their *ingredients and traits*.

Then, **a drink becomes a unique combination of ingredients**---espresso shots, portions of regular or specialty (almond, coconut, soy) milk, tea concentrates (chai or matcha), or smidges of chocolate. And don't forget **preparation techniques** (frothing to varying degrees) and **serving methods** (chilled or on ice).

Immediately, there's more precision to our thinking.

>This approach fundamentally changes how sales should be recorded.

Let's take the example of recording a sale for a drink called "*dirty chai*."

In my client's cash register system, it's recorded as a menu item of the same name.

On the menu, there's also an item called "*chai latte*."

By just looking at the names, we wouldn't be able to tell that a "*dirty chai*" is simply a "*chai latte*" with a shot of espresso.

>But if we were to forget *names* and think of *ingredients and traits* instead, a recorded sale in the abstract category "*dirty chai*" would become a sale of *an espresso shot, a portion of chai syrup, and a portion of frothed milk*, all taken separately.

Remember ["deconstructed coffee"](https://www.theage.com.au/national/victoria/deconstructed-coffee-melbourne-hipster-trend-has-social-media-steaming-20160602-gp9jrh.html) from Australia that took the Internet by storm a couple of years ago?

A Melbourne coffee shop became infamous for serving lattes like this:

![](C:\Coffee and Weather Code\deconstructedcoffee-619-386.jpg)

*In the world of "deconstructed coffee," the only way to say "thanks a latte" is passive-aggressive.*

It doesn't appear so at first, but **this "deconstructive" approach is very useful**.

>While this is no way to serve a product to a client, this is a perfect illustration of what the product looks like to the café's manager.

**Logistically speaking, every *ingredient* should be stocked, and machines should be operational to make sure the drink has proper *traits*.**

For example, take ~~[what was supposed to be a](https://www.theage.com.au/national/victoria/deconstructed-coffee-its-for-snobs-not-hipsters-says-café-behind-beaker-brew-20160602-gp9vw6.html)~~ the long macchiato above. 

>Normally, it consists of *an espresso shot*, made longer with *hot water*, and *milk*---*the ingredients*.

>The milk is also *frothed*---that's *a trait*. 

>The manager will stock coffee beans, which become espresso after passing through the grinder and the espresso machine.

>They will also restock the milk and make sure the frothing nozzle is always operational.

Aesthetics aside, the Melbourne barista's deconstructive approach was the approach of a responsible manager.

It took into account both *the logistics of the business* and *the highly customizable nature of the product*.

**I took the same approach to my dataset**.

>I was *not* concerned with how many _"long macchiatos"_ the café sold. 

>I wanted to know how many _espresso shots and milk portions_ went into the drinks sold.

I'll call this approach ***The Melbourne Deconstruction***.

This approach, where a drink is understood not as an entity representative of *one* category, but a unique product coded according to *several* defining features---ingredients and traits---would

* **help my data speak the language of my client's logistics** and deliver value beyond what is offered by their cash register system, which already tracks sales of drinks, but not the ingredients,

* **streamline the interpretation of results at the modeling stage**, and

* **make it easier to discuss my data with those unfamiliar with third wave coffee culture**.

It's good to know that something that was [made fun of on the Internet](https://mashable.com/2016/05/31/deconstructed-coffee/) can be used to our advantage in a data science project.

[It was the perfect target until it became the perfect weapon.](http://www.imdb.com/title/tt0258463/taglines)

[^1]: To respect the conditions of my non-disclosure agreement with the owner, I will not name the business. This means that I will not be able to share or publish the raw files, as they contain identifying information.
[^2]: The client is part of [the third wave coffee movement](http://achillescoffeeroasters.com/what-is-the-third-wave-coffee-movement/) in the coffee industry: an independent-minded, artisanal reaction to the corporate [second wave](https://www.craftbeveragejobs.com/the-history-of-first-second-and-third-wave-coffee-22315/) of the likes of Starbucks. The third wave coffee market treats coffee as a premium product defined by attention to detail in all stages of the process, from sourcing fair-trade beans to roasting to preparing quality espresso. By definition, these small businesses do not have access to big data that second-wave chains like Starbucks have. However, lately, the third wave industry has [attracted](https://achillescoffeeroasters.com/growth-of-third-wave-coffee/) large-scale investments. Putting aside the question of whether an overgrown third-wave coffee company can still be considered third wave, any third-wave coffee business will benefit from analyzing data available to it to thwart competition from both big chains and other small businesses.

# DATA WRANGLING

## PLANNING THE DATASET

To get **the raw sales data**, I asked my client to generate \*.csv files via the cloud-based cash register system they use.

The client exported 448 files. 

Each of the files detailed all orders made at the coffee shop in one calendar day.

>My goal was to pull the sales data from all 448 files into a single data frame.
>
>Then, I would combine it with hourly weather data.

Originally, I [proposed](https://github.com/friendelectric/Coffee-Weather/blob/master/Capstone%20Proposal%20Coffee%20and%20Weather.Rmd) that I would use *daily* weather data, combined with *daily* sales. 

However, after giving it some more thought, I decided to create a dataset of *hourly* sales combined with *hourly* weather data.

My reason for this was threefold:

* **Unlike the weather, the café does not operate 24/7.** It’s closed for at least ten hours every day, but the weather is still out there. The gaps in time would make the daily resolution weather data unrepresentative of the daily sales.

* **Hourly data is better suited for an immediate snapshot of the situation.** When I think of getting a coffee, I’m not thinking “what is the weather going to be like today, on average?” I’m thinking, “is it nice enough out to go grab a coffee, right now?” The hourly resolution is more representative of the consumer's decision making.

* **It would make for more than ten times as many observations.** 

## WRANGLING CAFÉ DATA

Since I cannot share the files I obtained from the client, I made [a file illustrating the formatting of the raw data]( https://github.com/friendelectric/Coffee-Weather/blob/master/sample-raw-file.csv).[^3]

[^3]: The raw files I worked with were in French. The names of items were changed. “\$\$\$” replaces price information. The file is abridged to show what the beginning and end of a raw file would look like.

Looking at **the raw data's formatting**, we see that

* the first lines always contain the name of the business and the calendar day for which the file details recorded orders,

* a section with reference information follows, explaining the fields that detail order information, and

* orders follow until the end of the file, separated by a series of dashes.

And **for every recorded order, there is**

*	*General information*: order number and time

*	*Price and payment information*: order sub-total before tax, calculated sales tax charges, taxable order total, order total for tax exempt orders, order total, payment type---cash or debit, device used to record the order

*	*Information on items ordered*:

    +	ID of menu item, name of the menu item, and price (e.g., “4, Large Latte, 2.83”)

    +	ID of menu item modifier, name of the modifier, and (not in all cases) price (e.g., “3, Extra shot, 0.48” or “1, Milk option: regular”)

>Since my analysis would concern drinks only, I would need to discard the data on food and services. 
>
>I would also discard the data on prices and payment.

To write the initial wrangling procedure, I used *readLines()* to extract lines from one file.[^4]

[^4]: A later version of this procedure can be found in a function  [here](https://github.com/friendelectric/Coffee-Weather/blob/master/1%20extract%20menu%20items%20and%20modifiers.R#L40).

I would feed one of the \*.csv files to my script, making a character vector with a sequence of order numbers and menu items ordered.

After examining printouts of this vector for different files, I realized **I was facing three big problems**:

1. **Most drinks could be modified in many ways. When looking at a record of the order, there was no way to tell what’s a menu item, and what is an option to a menu item.** Most menu items at the café can be ordered with different options, or modifiers. For example, a large latte can be made with an extra shot of espresso, or with specialty milk to replace regular milk. There could be any number of options recorded. Other than knowing that the first item in an order would not be a modifier, a script could not gauge by looking at the data which level of hierarchy each object is. The only unique thing about modifiers is that some of them don’t have prices, but most do. When I extracted all the ID-name pairs from several files, I concluded that there was also no regularity to the identification numbers. They appear to have been in the order in which the café’s manager recorded them into the system.

2. **Identification numbers were double-booked, referring to both menu items and options.** For example, 4 would refer to “large latte" and also to “soy milk.” This means that the same number can refer to two objects, each on a different level of the item-modifier hierarchy.

3. **At least several number-descriptor combinations---typos and promos---referred to the same menu item.** Some menu items in the data are typos―a result of human error when maintaining the menu in the cash register system. For example, "CappucciNNo" is a typo: it should be recorded as "CappucciNo." Still, because of the typo, it appears like a separate item called "CappucciNNo" was sold for a few days. Also, promotional versions of regular items were recorded as separate items. For instance, "ESPRESSO PROMO!!" isn’t a separate item and would need to be recorded as the rest of "espressos."

My solution to the problem of discerning between menu items and modifiers, as well as typos and promos, was to create *a reference table* where every ID-descriptor pair is coded according to its type and position in the menu-modifier hierarchy.

I would then expand the reference table into *a coding book*.

The coding book would be the key to performing *the Melbourne Deconstruction*: recording each drink sale as a unique combination of ingredients and traits.

>**I wrangled the client-supplied data in four steps**, each implemented in a separate R script:

1. [Learn what’s on the menu and put it into a reference table](https://github.com/friendelectric/Coffee-Weather/blob/master/1 extract menu items and modifiers.R). I first extracted all unique pairs of identification numbers and descriptors from all 448 files. Then, I manually coded each ID-descriptor pair according to its type (drink, food, or service), position in the menu-modifier hierarchy (noting whether it’s a standalone menu item, like a latte, or an option to a menu item, like soy milk), and correct spelling (if it was a typo or a promotional item). I would use this reference table later to differentiate among elements within a vector extracted from a raw \*.csv file.

2. [Extract data on all orders completed at the café into a data frame](https://github.com/friendelectric/Coffee-Weather/blob/master/2 extract orders.R). The data frame of orders includes ID number for the order, day, HH:MM, and the contents of the order stored as a string. 

3. [Using the reference table, discard unnecessary items (food and services)](https://github.com/friendelectric/Coffee-Weather/blob/master/3 discard foods and services.R) from the data frame of orders. After this was done, I expanded my reference table into a coding book. The coding book contains codes for every feature that I could potentially use in my statistical analysis. Most variables code TRUE or FALSE on the presence of a given ingredient or trait in a drink. For instance, a latte is coded TRUE on espresso, milk, and froth.

4. [Build a table of hourly counts of sales by drink feature](https://github.com/friendelectric/Coffee-Weather/blob/master/4 hourly counts.R). 

>**The final script was particularly complex and included several stages:**

1. I [parsed](https://github.com/friendelectric/Coffee-Weather/blob/master/4%20hourly%20counts.R#L56) my *data frame of orders* made at the café (one observation is an order that may contain several drinks), making it into a *data frame of drinks* (one observation is one drink: the base menu item and modifiers).

2. I [replaced](https://github.com/friendelectric/Coffee-Weather/blob/master/4%20hourly%20counts.R#L146) typos and promos with the menu items' original names.

3. Then, I [joined](https://github.com/friendelectric/Coffee-Weather/blob/master/4%20hourly%20counts.R#L184) the data frame of drinks with the coding book, adding the variables for coding the drinks on traits and ingredients.

4. Having the data (in string form) on what modifiers were used and the default codes of base drinks, I [overwrote](https://github.com/friendelectric/Coffee-Weather/blob/master/4%20hourly%20counts.R#L197) the default codes with relevant modifiers’ codes. For instance, having your latte “decaf” would cancel out the “espresso,” or having it “iced” would cancel out “frothed.”

5. Finally, I [cleaned](https://github.com/friendelectric/Coffee-Weather/blob/master/4%20hourly%20counts.R#L309) the resulting data frame, [counted](https://github.com/friendelectric/Coffee-Weather/blob/master/4%20hourly%20counts.R#L391) tallies for hourly sales of each ingredient and trait, [wrangled](https://github.com/friendelectric/Coffee-Weather/blob/master/4%20hourly%20counts.R#L440) the weather data for the client’s location, and [joined](https://github.com/friendelectric/Coffee-Weather/blob/master/4%20hourly%20counts.R#L489) it with the dataset of drink sales.

## WRANGLING WEATHER DATA

To obtain the weather data, I initially tried using the [weatherData](https://github.com/Ram-N/weatherData) package, which pulls data from [wunderground.com](http://wunderground.com/). However, the website changed the URL directory structure that the package relied on. When I was trying to pull the data, wunderground.com responded with "bad request," and the issue [wasn't fixed](https://github.com/Ram-N/weatherData/issues/39#issuecomment-353752692) yet. 

I considered other R packages, but most of them seemed to require getting API keys from the weather data providers they connected to.

This seemed unnecessarily time-consuming, compared to the option I [proposed](https://github.com/friendelectric/Coffee-Weather/blob/master/Capstone%20Proposal%20Coffee%20and%20Weather.Rmd) initially---using government-supplied data from Climate Canada.

The agency had \*.csv files available for download with no special arrangements needed. 

For the weather station in my client's location, reports were available at [daily](http://climate.weather.gc.ca/climate_data/daily_data_e.html?hlyRange=2013-02-13%7C2017-11-27&dlyRange=2013-02-14%7C2017-11-27&mlyRange=%7C&StationID=51157&Prov=QC&urlExtension=_e.html&searchType=stnProx&optLimit=yearRange&Month=11&Day=27&StartYear=2014&EndYear=2017&Year=2017&selRowPerPage=25&Line=1&txtRadius=25&optProxType=city&selCity=45%7C31%7C73%7C39%7CMontr%C3%A9al&selPark=&txtCentralLatDeg=&txtCentralLatMin=0&txtCentralLatSec=0&txtCentralLongDeg=&txtCentralLongMin=0&txtCentralLongSec=0&timeframe=2) and [hourly](http://climate.weather.gc.ca/climate_data/hourly_data_e.html?hlyRange=2013-02-13%7C2018-02-07&dlyRange=2013-02-14%7C2018-02-07&mlyRange=%7C&StationID=51157&Prov=QC&urlExtension=_e.html&searchType=stnName&optLimit=yearRange&StartYear=1840&EndYear=2018&selRowPerPage=25&Line=0&searchMethod=contains&txtStationName=montreal+intl+a&timeframe=1&Year=2018&Month=1&Day=3) resolution. 

For my purposes, downloading government data proved to be faster and more straightforward than using packages.

The wrangling procedures I performed on the weather data were less ~~nightmare-inducing~~ exciting than what I had to do for the sales data, but they can be found in [my last wrangling script](https://github.com/friendelectric/Coffee-Weather/blob/master/4%20hourly%20counts.R#L440).

# DIVING INTO THE DATASET

## OVERVIEW

The dataset can be downloaded [here](https://github.com/friendelectric/Coffee-Weather/blob/master/CafeHourly.csv).

Before going deeper into the dataset, let's look at its structure:

```{r include=FALSE}
setwd('C:/Coffee and Weather Code/data')
dataset <- read.csv("CafeHourly.csv")
# Ensuring correct types for time parameters:
dataset$Day    <- as.Date(dataset$Day)
dataset$Hour   <- as.integer(dataset$Hour)
dataset$Season <- as.factor(dataset$Season)
```
```{r}
str(dataset)
```

Every observation is *one hour of the cafe's operations*.

Each of the *Content.\** and *Trait.\** variables represents the number of drinks sold that had the noted ingredient or trait.

For example, an observation can be partially spelled out like this:

```
On September 1st, 2016, from 10:00 to 10:59AM, 
the cafe sold 6 espresso-based drinks, 
6 drinks containing milk, 1 drink containing chocolate, 
0 drinks that were chilled or iced, and 7 frothed drinks. 
During that time, the sky was clear, there were no fog, rain, or snow, 
and the temperature was at 20.5 degrees C.
```

The next two sections describe the dependent and independent variables in detail.

## DEPENDENT VARIABLES

All dependent variables other than *DrinksSold* and *Size.Mean* describe the number of drinks adhering to the noted feature that were sold during the hour of observation.

The following table provides context on the coding process, noting examples of drinks that were coded as TRUE on the relevant features (ingredients and traits).

Variable               | Description
-----------------------|------------------------------------------------------
*DrinksSold*           | Total number of drinks sold.
*Size.Mean*            | Mean volume of drinks sold, in milliliters.
*Content.Water*        | Water understood as a separate ingredient added to the drink (e.g., americanos are made by pouring an espresso shot in a cup of hot water). This excludes water going through an espresso machine, filter pot, chemex or other pour over devices.
*Content.Tea*          | Tea leaves, chai, and matcha, as well as kombucha (fermented tea).
*Content.RegularMilk*  | Regular milk is the default option for making espresso-based beverages (e.g., lattes, cappuccinos). In a hot drink, the milk is frothed. In a cold drink, it's used as is.
*Content.SpecialtyMilk*| For an additional price, clients have the option to substitute regular milk for specialty milks (coconut, almond, or soy). Appeals particularly to the lactose intolerant. However, many others prefer the taste of a certain specialty milk to the regular milk.
*Content.Chocolate*    | Found in hot chocolates (which have quite a large portion of chocolate) and mochas (lattes with a smidge of chocolate on the bottom to offset the bitterness of espresso).
*Content.Seasonal*     | Seasonal ingredients added to standard drinks (e.g., latte), like pumpkin spice (fall) or mint (winter).
*Content.Juice*        | Bottled juices.
*Trait.HighInSugar*    | Drinks high in sugar, compared to other drinks offered at the shop. Primarily, drinks with chai syrup.
*Trait.HighInCaffeine* | Drinks with extra espresso shots or filtered/drip/slow bar coffee. For a discussion on the concentration of caffeine in espresso vs. drip coffee, see [this post](https://www.kickinghorsecoffee.com/en/blog/caffeine-myths-espresso-vs-drip).
*Trait.Cold*           | Drinks served on ice, as well as cold brew coffee or any bottled or canned drinks served from the fridge.
*Trait.Froth*          | Hot drinks with milk, prepared using a frothing nozzle on an espresso machine.

## INDEPENDENT VARIABLES

The dataset contains the following weather variables:

* [Temperature](http://climate.weather.gc.ca/glossary_e.html#temp), degrees Celsius

* [Dew Point temperature](http://climate.weather.gc.ca/glossary_e.html#dewPnt), degrees Celsius

* [Relative humidity](http://climate.weather.gc.ca/glossary_e.html#r_humidity), percent

* [Wind speed](http://climate.weather.gc.ca/glossary_e.html#windSpd), km/h

* [Pressure](http://climate.weather.gc.ca/glossary_e.html#stnPre), kilopascals

* Binaries for [atmospheric phenomena and sky conditions](http://climate.weather.gc.ca/glossary_e.html#weather)---rain, snow, fog, and cloudy

## IMPLICATIONS

The dataset can have implications to several areas of the client’s day-to-day operations:

* **Stock management.** Having insight into the effects of weather variables on product demand, the client can adjust stock management. For example, they could order less milk if they expect less demand for milk-based beverages given tomorrow’s weather forecast.

* **Product development.** Knowing how weather conditions influence beverage preferences, the client may decide to adjust the current product selection or create new products. To illustrate, knowing that there is a better chance of selling a chocolaty beverage when it rains may lead to developing a special drink containing chocolate only sold on rainy days.

* **Marketing strategies.** Aware of just which products customers want to buy in certain weather conditions, the client may adjust what―and how―they market. Weather-based marketing won’t just involve deciding what witty message will work well on that sidewalk sign outside the door. For instance, the client may want to collect their customers’ email addresses to produce weather-trigged marketing campaigns or send out a timely call to action on the business’ social media accounts.

Importantly, **the dataset can only be used to answer research questions about my client's business, and not the third wave café market or the coffee industry in general**.

## PRELIMINARY EXPLORATION

**While my inquiry will be focused on the relationship between weather and sales counts, the dataset can be useful to my client in other ways.**

My dataset is cleaner and more nuanced than what the client has access to via their cash register system.

**It may let us spot trends** that the client may not have been aware of.

For instance, this plot shows the number of filtered coffee drinks sold each hour, by season:

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
Spring <- dataset %>% filter(Season=="Spring")
Fall <- dataset %>% filter(Season=="Fall")
Winter <- dataset %>% filter(Season=="Winter")
Summer <- dataset %>% filter(Season=="Summer")
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
library(cowplot)

A<-ggplot(Spring, aes(x=Hour, y=Content.Drip)) + geom_jitter(color="dark green", alpha=.75) + geom_smooth(color="green") + ggtitle("Spring") + theme_minimal() + 
  xlim(7, 20) + ylim(0,10)

B<-ggplot(Fall, aes(x=Hour, y=Content.Drip)) + geom_jitter(color="orange", alpha=.75) + geom_smooth(color="deeppink2") + ggtitle("Fall") + theme_minimal() + 
  xlim(7, 20) + ylim(0,10) + 
  theme(axis.text.x = element_blank(), axis.title.x = element_blank())

C<-ggplot(Winter, aes(x=Hour, y=Content.Drip)) + geom_jitter(color="blue", alpha=.75) + geom_smooth(color="light blue") + ggtitle("Winter") + theme_minimal() + 
  xlim(7, 20) + ylim(0,10) + 
  theme(axis.text.x = element_blank(), 
        axis.text.y = element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank())

D<-ggplot(Summer, aes(x=Hour, y=Content.Drip)) + geom_jitter(color="yellowgreen", alpha=.75) + geom_smooth(color="dark green") + ggtitle("Summer") + theme_minimal() + 
  xlim(7, 20) + ylim(0,10)+ 
  theme(axis.text.y = element_blank(),
        axis.title.y = element_blank()) 

plot_grid(B,C,A,D)
```

Judging by this, it's probably not a good idea to start brewing a pot of filtered coffee after 5PM, unless there are orders coming in---particularly in the summertime.

Finally, **if needed, the dataset can be rescaled to the daily resolution**, providing a further dimension of insight for the client.

```{r}
daily <- dataset %>% group_by(Day) %>% 
  summarise(sum(Trait.Cold), sum(Trait.HighInCaffeine), mean(Temperature))
```
```{r echo=FALSE}
daily <- as.data.frame(daily)
colnames(daily) <- c("Day", "ColdDrinks", "HighInCaffeine", "MeanTemp")
```

For example, we could plot daily **time series** for drinks with specific features, like *cold* or *high in caffeine*:

```{r echo=FALSE}
CLD <- ggplot(daily, aes(x=Day, y=ColdDrinks)) + geom_line() + 
  scale_colour_gradient(low="blue", high="red") 
CAF <- ggplot(daily, aes(x=Day, y=HighInCaffeine)) + geom_line()
plot_grid(CLD, CAF, nrow=2, ncol=1)
```

These are just some possibilities of what can be done with the dataset besides analyzing the impact of weather variables!